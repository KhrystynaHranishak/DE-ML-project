{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "ML_project",
      "language": "python",
      "name": "ml_project"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "name": "Fine_tuning_BERT.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "462eeaa81da44a5684bb776b45dbf5e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_a66d027bec2d460faf2e3f19d8a09437",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_b334d60f665f45f8bbbb60ff0c2fe554",
              "IPY_MODEL_3d4affd0bb9e40858f2bcb3c4f7a9afb"
            ]
          }
        },
        "a66d027bec2d460faf2e3f19d8a09437": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b334d60f665f45f8bbbb60ff0c2fe554": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_c9a7da8441eb4335bc06c1727e6c099d",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 231508,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 231508,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e7d83c23f79e4aa0a5afa18655ee6a87"
          }
        },
        "3d4affd0bb9e40858f2bcb3c4f7a9afb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_0bcc1af0b69f4b0384b3e06e9b8209c5",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 232k/232k [00:05&lt;00:00, 45.6kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_70e791b967ed4724b3d81c8f1ce6bae9"
          }
        },
        "c9a7da8441eb4335bc06c1727e6c099d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e7d83c23f79e4aa0a5afa18655ee6a87": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0bcc1af0b69f4b0384b3e06e9b8209c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "70e791b967ed4724b3d81c8f1ce6bae9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0c5dbb3fd113426b8a8042f8970cef5a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_b821fa98135644ed8445f8f636d65704",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_8da0f306c8b44b80834b993c7445e9ec",
              "IPY_MODEL_12471219ed024ba2b22f1e0e20e36d17"
            ]
          }
        },
        "b821fa98135644ed8445f8f636d65704": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8da0f306c8b44b80834b993c7445e9ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_fefdc36721cf4da7998903b76ec1e049",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 28,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 28,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c9e9d5aced8d47f38a18656eaf779368"
          }
        },
        "12471219ed024ba2b22f1e0e20e36d17": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_3460ce3dc80e4c3d8d7515379334fab1",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 28.0/28.0 [00:02&lt;00:00, 13.1B/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e6e10b25211c4e338f1869a6b9947d77"
          }
        },
        "fefdc36721cf4da7998903b76ec1e049": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c9e9d5aced8d47f38a18656eaf779368": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3460ce3dc80e4c3d8d7515379334fab1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e6e10b25211c4e338f1869a6b9947d77": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "62372064d8f34b7c866a6ce3031b8c16": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_603dea13eeb941ab9be664d988c3a081",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_215f1ac6cf9b422a9a96b766cd84770d",
              "IPY_MODEL_89666fe61d6a4e1ab1099ce5e793ffa1"
            ]
          }
        },
        "603dea13eeb941ab9be664d988c3a081": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "215f1ac6cf9b422a9a96b766cd84770d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_dd789ca9167d4a99b6c7e4508b1f5286",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 466062,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 466062,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_87d055d4368b4136aaa0e3b9dd636fad"
          }
        },
        "89666fe61d6a4e1ab1099ce5e793ffa1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_8b80f1ab7ff2410993aa8e85d34440e2",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 466k/466k [00:00&lt;00:00, 500kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6479f1949e284433a709e79fb739dc20"
          }
        },
        "dd789ca9167d4a99b6c7e4508b1f5286": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "87d055d4368b4136aaa0e3b9dd636fad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8b80f1ab7ff2410993aa8e85d34440e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6479f1949e284433a709e79fb739dc20": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "764f5c957aac4a3aa17129b3dbf2ddbf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_a374397773414c8f961a7b6eb1931cc9",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_e2585e324b264253a63090c98479bdc2",
              "IPY_MODEL_fe84841643424232a056e2d33cd37739"
            ]
          }
        },
        "a374397773414c8f961a7b6eb1931cc9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e2585e324b264253a63090c98479bdc2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_6b069c53b2b44d7ea45e8472c44f1881",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 433,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 433,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_72e2f74f3e234735b8a2816a8231b815"
          }
        },
        "fe84841643424232a056e2d33cd37739": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_457b9a37e58f40699d2eb417904ef689",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 433/433 [00:00&lt;00:00, 705B/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f8352bed00c943b3814507c0ac00db89"
          }
        },
        "6b069c53b2b44d7ea45e8472c44f1881": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "72e2f74f3e234735b8a2816a8231b815": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "457b9a37e58f40699d2eb417904ef689": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f8352bed00c943b3814507c0ac00db89": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c55f8a845fd546bc938cd41a79c03417": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_ee2eae94e6ec46fdbe043ed19468e81a",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_d778b7e2dc6945eebafe1c0a8b2359df",
              "IPY_MODEL_05afec2284fa4a3cb9bf8f20c9725b75"
            ]
          }
        },
        "ee2eae94e6ec46fdbe043ed19468e81a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d778b7e2dc6945eebafe1c0a8b2359df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_170c76bd3a4a44a0ba1b21bf12d4448e",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 440473133,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 440473133,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_201aa52a6a2745dfbd07d35e3ae62a9c"
          }
        },
        "05afec2284fa4a3cb9bf8f20c9725b75": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_b9443c6341ee4986998fe5de4c0bddea",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 440M/440M [00:13&lt;00:00, 33.7MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c981ac2acb1b42fc8b81e82fa304eb1d"
          }
        },
        "170c76bd3a4a44a0ba1b21bf12d4448e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "201aa52a6a2745dfbd07d35e3ae62a9c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b9443c6341ee4986998fe5de4c0bddea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c981ac2acb1b42fc8b81e82fa304eb1d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z0R55D1UaMaU",
        "outputId": "065d7472-ab74-4463-d1b8-5f53b94b9cd9"
      },
      "source": [
        "! pip install transformers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ed/d5/f4157a376b8a79489a76ce6cfe147f4f3be1e029b7144fa7b8432e8acb26/transformers-4.4.2-py3-none-any.whl (2.0MB)\n",
            "\u001b[K     |████████████████████████████████| 2.0MB 4.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/04/5b870f26a858552025a62f1649c20d29d2672c02ff3c3fb4c688ca46467a/tokenizers-0.10.2-cp37-cp37m-manylinux2010_x86_64.whl (3.3MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3MB 17.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/08/cd/342e584ee544d044fb573ae697404ce22ede086c9e87ce5960772084cad0/sacremoses-0.0.44.tar.gz (862kB)\n",
            "\u001b[K     |████████████████████████████████| 870kB 32.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.44-cp37-none-any.whl size=886084 sha256=6d952c0a92d2d4d8911fcb687a0a4b63f3253661e492822afa3eb88f7f7bc721\n",
            "  Stored in directory: /root/.cache/pip/wheels/3e/fb/c0/13ab4d63d537658f448366744654323077c4d90069b6512f3c\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.44 tokenizers-0.10.2 transformers-4.4.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qxC-jKSDZ8hK",
        "outputId": "bc205aea-cd2e-449a-904e-1586d32b7da0"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o0VUJgvYZ8hU"
      },
      "source": [
        "### Solution based on language model: fine-tuning BERT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LTrkztaxZ8hV"
      },
      "source": [
        "import pandas as pd\n",
        "from IPython import display\n",
        "from pprint import pprint\n",
        "import re\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "\n",
        "from tensorflow.keras import models, layers, preprocessing as kprocessing\n",
        "from tensorflow.keras import backend as K\n",
        "import transformers\n",
        "\n",
        "def printmd(s):\n",
        "    display.display(display.Markdown(s))\n",
        "    \n",
        "sns.set_style('darkgrid')\n",
        "sns.set_palette(\"pastel\")"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_R881l8pZ8hW",
        "outputId": "316e3a6c-b2e7-42e9-e472-3f6f6ade468b"
      },
      "source": [
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():       \n",
        "    device = torch.device(\"cuda\")\n",
        "    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n",
        "    print('Device name:', torch.cuda.get_device_name(0))\n",
        "\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 1 GPU(s) available.\n",
            "Device name: Tesla K80\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GS856tfdZ8hY"
      },
      "source": [
        "# upload data\n",
        "PATH = '/content/drive/MyDrive/ColabNotebooks/DE_ML_project/'\n",
        "df_train = pd.read_pickle(PATH + 'Data/train_data.pkl')\n",
        "df_validation = pd.read_pickle(PATH + 'Data/test_data.pkl') # 30% of initial train.csv data\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nq1bW6ap0CGy"
      },
      "source": [
        "df_test = pd.read_pickle(PATH + 'Data/validation_data.pkl')"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UASzl7yFY8xE",
        "outputId": "a42eff2b-f8b1-433a-f3de-c1d0cef6617a"
      },
      "source": [
        "df_test['toxic'].value_counts()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    57888\n",
              "1     6090\n",
              "Name: toxic, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UXhn91n_Z8hZ"
      },
      "source": [
        "X_train = df_train.comment_text.values\n",
        "y_train = df_train.toxic.values\n",
        "\n",
        "X_val = df_validation.comment_text.values\n",
        "y_val = df_validation.toxic.values\n",
        "\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2K3yAvyY0cN7"
      },
      "source": [
        "X_test = df_test.comment_text.values\n",
        "y_test = df_test.toxic.values"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AXYKWWtmZ8hZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164,
          "referenced_widgets": [
            "462eeaa81da44a5684bb776b45dbf5e8",
            "a66d027bec2d460faf2e3f19d8a09437",
            "b334d60f665f45f8bbbb60ff0c2fe554",
            "3d4affd0bb9e40858f2bcb3c4f7a9afb",
            "c9a7da8441eb4335bc06c1727e6c099d",
            "e7d83c23f79e4aa0a5afa18655ee6a87",
            "0bcc1af0b69f4b0384b3e06e9b8209c5",
            "70e791b967ed4724b3d81c8f1ce6bae9",
            "0c5dbb3fd113426b8a8042f8970cef5a",
            "b821fa98135644ed8445f8f636d65704",
            "8da0f306c8b44b80834b993c7445e9ec",
            "12471219ed024ba2b22f1e0e20e36d17",
            "fefdc36721cf4da7998903b76ec1e049",
            "c9e9d5aced8d47f38a18656eaf779368",
            "3460ce3dc80e4c3d8d7515379334fab1",
            "e6e10b25211c4e338f1869a6b9947d77",
            "62372064d8f34b7c866a6ce3031b8c16",
            "603dea13eeb941ab9be664d988c3a081",
            "215f1ac6cf9b422a9a96b766cd84770d",
            "89666fe61d6a4e1ab1099ce5e793ffa1",
            "dd789ca9167d4a99b6c7e4508b1f5286",
            "87d055d4368b4136aaa0e3b9dd636fad",
            "8b80f1ab7ff2410993aa8e85d34440e2",
            "6479f1949e284433a709e79fb739dc20"
          ]
        },
        "outputId": "f4024ba5-6e99-4f41-d2ab-f3826cdc8ada"
      },
      "source": [
        "tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "462eeaa81da44a5684bb776b45dbf5e8",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0c5dbb3fd113426b8a8042f8970cef5a",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=28.0, style=ProgressStyle(description_w…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "62372064d8f34b7c866a6ce3031b8c16",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=466062.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D8aZNKJiZ8ha"
      },
      "source": [
        "def text_preprocessing(text):\n",
        "    text = re.sub(r'(@.*?)[\\s]', ' ', text)\n",
        "    text = re.sub(r'[0-9]+' , '' ,text)\n",
        "    text = re.sub(r'\\s([@][\\w_-]+)', '', text).strip()\n",
        "    text = re.sub(r'&amp;', '&', text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    text = text.replace(\"#\" , \" \")\n",
        "    encoded_string = text.encode(\"ascii\", \"ignore\")\n",
        "    decode_string = encoded_string.decode()\n",
        "    return decode_string\n",
        "\n",
        "\n",
        "def preprocessing_for_bert(data):\n",
        "    input_ids = []\n",
        "    attention_masks = []\n",
        "\n",
        "    # For every sentence...\n",
        "    for sent in data:\n",
        "        encoded_sent = tokenizer.encode_plus(\n",
        "            text=text_preprocessing(sent),  # Preprocess sentence\n",
        "            add_special_tokens=True,        # Add `[CLS]` and `[SEP]`\n",
        "            max_length=MAX_LEN,                  # Max length to truncate/pad\n",
        "            pad_to_max_length=True,         # Pad sentence to max length\n",
        "            #return_tensors='pt',           # Return PyTorch tensor\n",
        "            truncation = True,\n",
        "            return_attention_mask=True      # Return attention mask\n",
        "            )\n",
        "        input_ids.append(encoded_sent.get('input_ids'))\n",
        "        attention_masks.append(encoded_sent.get('attention_mask'))\n",
        "\n",
        "    # Convert lists to tensors\n",
        "    input_ids = torch.tensor(input_ids)\n",
        "    attention_masks = torch.tensor(attention_masks)\n",
        "\n",
        "    return input_ids, attention_masks"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        },
        "id": "0HbVnE2eZ8ha",
        "outputId": "3f2ed1cc-767e-4d31-e8b9-c2daea2d0bfa"
      },
      "source": [
        "MAX_LEN = 300\n",
        "\n",
        "printmd('**Example**')\n",
        "token_ids = list(preprocessing_for_bert([X_train[0]])[0].squeeze().numpy())\n",
        "print('Original: ', X_train[0])\n",
        "print('Token IDs: ', token_ids)\n",
        "\n",
        "# Run function `preprocessing_for_bert` on the train set and the validation set\n",
        "print('Tokenizing data...')\n",
        "train_inputs, train_masks = preprocessing_for_bert(X_train)\n",
        "val_inputs, val_masks = preprocessing_for_bert(X_val)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/markdown": "**Example**",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2074: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Original:  \"::No.  The reason some excellent editors leave is when reasoned and fully explained, perfectly valid edits are opposed by pigheaded people with personal motives  as in this case.  Your response to my inclusion of both photos has been purely ad hominem  no reasonable explanation given, only puerile, and boringly repetitive name calling.  Curious.  I see another, white editor has weighed in and supported the inclusion of the photograph of the white woman.  What?  You gonna call him \"\"racist scum,\"\" too?  It seems to me that shrill, incivil criticism of an edit based solely on the ethnic identity of the contributor is the only racism here. *x*  \n",
            "\n",
            "\"\n",
            "Token IDs:  [101, 1000, 1024, 1024, 2053, 1012, 1996, 3114, 2070, 6581, 10195, 2681, 2003, 2043, 24557, 1998, 3929, 4541, 1010, 6669, 9398, 10086, 2015, 2024, 4941, 2011, 10369, 4974, 2098, 2111, 2007, 3167, 17108, 2004, 1999, 2023, 2553, 1012, 2115, 3433, 2000, 2026, 10502, 1997, 2119, 7760, 2038, 2042, 11850, 4748, 7570, 11233, 2213, 2053, 9608, 7526, 2445, 1010, 2069, 16405, 11124, 2571, 1010, 1998, 11771, 2135, 23563, 2171, 4214, 1012, 8025, 1012, 1045, 2156, 2178, 1010, 2317, 3559, 2038, 12781, 1999, 1998, 3569, 1996, 10502, 1997, 1996, 9982, 1997, 1996, 2317, 2450, 1012, 2054, 1029, 2017, 6069, 2655, 2032, 1000, 1000, 16939, 8040, 2819, 1010, 1000, 1000, 2205, 1029, 2009, 3849, 2000, 2033, 2008, 28349, 1010, 4297, 12848, 4014, 6256, 1997, 2019, 10086, 2241, 9578, 2006, 1996, 5636, 4767, 1997, 1996, 12130, 2003, 1996, 2069, 14398, 2182, 1012, 1008, 1060, 1008, 1000, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "Tokenizing data...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SG6Fi29y0h0v",
        "outputId": "59478fa6-6eea-4552-9439-d542da788ee2"
      },
      "source": [
        "test_inputs, test_masks = preprocessing_for_bert(X_test)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2074: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M3Y4nP9gZ8hb"
      },
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "# Convert other data types to torch.Tensor\n",
        "train_labels = torch.tensor(y_train)\n",
        "val_labels = torch.tensor(y_val)\n",
        "\n",
        "# For fine-tuning BERT, the authors recommend a batch size of 16 or 32.\n",
        "batch_size = 16\n",
        "\n",
        "# Create the DataLoader for our training set\n",
        "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "# Create the DataLoader for our validation set\n",
        "val_data = TensorDataset(val_inputs, val_masks, val_labels)\n",
        "val_sampler = SequentialSampler(val_data)\n",
        "val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4iejkoH1Z8hc",
        "outputId": "4de8c7f9-5d84-443e-9583-535a4b41ed33"
      },
      "source": [
        "%time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import BertModel\n",
        "\n",
        "# Create the BertClassfier class\n",
        "class BertClassifier(nn.Module):\n",
        "    def __init__(self, freeze_bert=False):\n",
        "        super(BertClassifier, self).__init__()\n",
        "        # Specify hidden size of BERT, hidden size of our classifier, and number of labels\n",
        "        D_in, H, D_out = 768, 50, 2\n",
        "\n",
        "        # Instantiate BERT model\n",
        "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
        "        # self.LSTM = nn.LSTM(D_in,D_in,bidirectional=True)\n",
        "        # self.clf = nn.Linear(D_in*2,2)\n",
        "\n",
        "        # Instantiate an one-layer feed-forward classifier\n",
        "        self.classifier = nn.Sequential(\n",
        "            # nn.LSTM(D_in,D_in)\n",
        "            nn.Linear(D_in, H),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(H, D_out)\n",
        "        )\n",
        "\n",
        "        # Freeze the BERT model\n",
        "        if freeze_bert:\n",
        "            for param in self.bert.parameters():\n",
        "                param.requires_grad = False\n",
        "        \n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        # Feed input to BERT\n",
        "        outputs = self.bert(input_ids=input_ids,\n",
        "                            attention_mask=attention_mask)\n",
        "        # Extract the last hidden state of the token `[CLS]` for classification task\n",
        "        last_hidden_state_cls = outputs[0][:, 0, :]\n",
        "        # Feed input to classifier to compute logits\n",
        "        logits = self.classifier(last_hidden_state_cls)\n",
        "\n",
        "        return logits"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 4 µs, sys: 0 ns, total: 4 µs\n",
            "Wall time: 8.34 µs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A4KyEFQzbwYh"
      },
      "source": [
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "\n",
        "def initialize_model(epochs=4):\n",
        "    \"\"\"Initialize the Bert Classifier, the optimizer and the learning rate scheduler.\n",
        "    \"\"\"\n",
        "    # Instantiate Bert Classifier\n",
        "    bert_classifier = BertClassifier(freeze_bert=False)\n",
        "\n",
        "    # Tell PyTorch to run the model on GPU\n",
        "    bert_classifier.to(device)\n",
        "\n",
        "    # Create the optimizer\n",
        "    optimizer = AdamW(bert_classifier.parameters(),\n",
        "                      lr=5e-5,    # Default learning rate\n",
        "                      eps=1e-8    # Default epsilon value\n",
        "                      )\n",
        "\n",
        "    # Total number of training steps\n",
        "    total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "    # Set up the learning rate scheduler\n",
        "    scheduler = get_linear_schedule_with_warmup(optimizer,\n",
        "                                                num_warmup_steps=0, # Default value\n",
        "                                                num_training_steps=total_steps)\n",
        "    return bert_classifier, optimizer, scheduler"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "avdn3GCebwbo"
      },
      "source": [
        "import random\n",
        "import time\n",
        "\n",
        "# Specify loss function\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "def set_seed(seed_value=42):\n",
        "    \"\"\"Set seed for reproducibility.\n",
        "    \"\"\"\n",
        "    random.seed(seed_value)\n",
        "    np.random.seed(seed_value)\n",
        "    torch.manual_seed(seed_value)\n",
        "    torch.cuda.manual_seed_all(seed_value)\n",
        "\n",
        "def train(model, train_dataloader, val_dataloader=None, epochs=4, evaluation=False):\n",
        "    \"\"\"Train the BertClassifier model.\n",
        "    \"\"\"\n",
        "    # Start training loop\n",
        "    print(\"Start training...\\n\")\n",
        "    for epoch_i in range(epochs):\n",
        "        # =======================================\n",
        "        #               Training\n",
        "        # =======================================\n",
        "        # Print the header of the result table\n",
        "        print(f\"{'Epoch':^7} | {'Batch':^7} | {'Train Loss':^12} | {'Val Loss':^10} | {'Val Acc':^9} | {'Elapsed':^9}\")\n",
        "        print(\"-\"*70)\n",
        "\n",
        "        # Measure the elapsed time of each epoch\n",
        "        t0_epoch, t0_batch = time.time(), time.time()\n",
        "\n",
        "        # Reset tracking variables at the beginning of each epoch\n",
        "        total_loss, batch_loss, batch_counts = 0, 0, 0\n",
        "\n",
        "        # Put the model into the training mode\n",
        "        model.train()\n",
        "\n",
        "        # For each batch of training data...\n",
        "        for step, batch in enumerate(train_dataloader):\n",
        "            batch_counts +=1\n",
        "            # Load batch to GPU\n",
        "            b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n",
        "\n",
        "            # Zero out any previously calculated gradients\n",
        "            model.zero_grad()\n",
        "\n",
        "            # Perform a forward pass. This will return logits.\n",
        "            logits = model(b_input_ids, b_attn_mask)\n",
        "\n",
        "            # Compute loss and accumulate the loss values\n",
        "            loss = loss_fn(logits, b_labels)\n",
        "            batch_loss += loss.item()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Perform a backward pass to calculate gradients\n",
        "            loss.backward()\n",
        "\n",
        "            # Clip the norm of the gradients to 1.0 to prevent \"exploding gradients\"\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "            # Update parameters and the learning rate\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            # Print the loss values and time elapsed for every 20 batches\n",
        "            if (step % 20 == 0 and step != 0) or (step == len(train_dataloader) - 1):\n",
        "                # Calculate time elapsed for 20 batches\n",
        "                time_elapsed = time.time() - t0_batch\n",
        "\n",
        "                # Print training results\n",
        "                print(f\"{epoch_i + 1:^7} | {step:^7} | {batch_loss / batch_counts:^12.6f} | {'-':^10} | {'-':^9} | {time_elapsed:^9.2f}\")\n",
        "\n",
        "                # Reset batch tracking variables\n",
        "                batch_loss, batch_counts = 0, 0\n",
        "                t0_batch = time.time()\n",
        "\n",
        "        # Calculate the average loss over the entire training data\n",
        "        avg_train_loss = total_loss / len(train_dataloader)\n",
        "\n",
        "        print(\"-\"*70)\n",
        "        # =======================================\n",
        "        #               Evaluation\n",
        "        # =======================================\n",
        "        if evaluation == True:\n",
        "            # After the completion of each training epoch, measure the model's performance\n",
        "            # on our validation set.\n",
        "            val_loss, val_accuracy = evaluate(model, val_dataloader)\n",
        "\n",
        "            # Print performance over the entire training data\n",
        "            time_elapsed = time.time() - t0_epoch\n",
        "            \n",
        "            print(f\"{epoch_i + 1:^7} | {'-':^7} | {avg_train_loss:^12.6f} | {val_loss:^10.6f} | {val_accuracy:^9.2f} | {time_elapsed:^9.2f}\")\n",
        "            print(\"-\"*70)\n",
        "        print(\"\\n\")\n",
        "    \n",
        "    print(\"Training complete!\")\n",
        "\n",
        "\n",
        "def evaluate(model, val_dataloader):\n",
        "    \"\"\"After the completion of each training epoch, measure the model's performance\n",
        "    on our validation set.\n",
        "    \"\"\"\n",
        "    # Put the model into the evaluation mode. The dropout layers are disabled during\n",
        "    # the test time.\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables\n",
        "    val_accuracy = []\n",
        "    val_loss = []\n",
        "\n",
        "    # For each batch in our validation set...\n",
        "    for batch in val_dataloader:\n",
        "        # Load batch to GPU\n",
        "        b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n",
        "\n",
        "        # Compute logits\n",
        "        with torch.no_grad():\n",
        "            logits = model(b_input_ids, b_attn_mask)\n",
        "\n",
        "        # Compute loss\n",
        "        loss = loss_fn(logits, b_labels)\n",
        "        val_loss.append(loss.item())\n",
        "\n",
        "        # Get the predictions\n",
        "        preds = torch.argmax(logits, dim=1).flatten()\n",
        "\n",
        "        # Calculate the accuracy rate\n",
        "        accuracy = (preds == b_labels).cpu().numpy().mean() * 100\n",
        "        val_accuracy.append(accuracy)\n",
        "\n",
        "    # Compute the average accuracy and loss over the validation set.\n",
        "    val_loss = np.mean(val_loss)\n",
        "    val_accuracy = np.mean(val_accuracy)\n",
        "\n",
        "    return val_loss, val_accuracy"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "764f5c957aac4a3aa17129b3dbf2ddbf",
            "a374397773414c8f961a7b6eb1931cc9",
            "e2585e324b264253a63090c98479bdc2",
            "fe84841643424232a056e2d33cd37739",
            "6b069c53b2b44d7ea45e8472c44f1881",
            "72e2f74f3e234735b8a2816a8231b815",
            "457b9a37e58f40699d2eb417904ef689",
            "f8352bed00c943b3814507c0ac00db89",
            "c55f8a845fd546bc938cd41a79c03417",
            "ee2eae94e6ec46fdbe043ed19468e81a",
            "d778b7e2dc6945eebafe1c0a8b2359df",
            "05afec2284fa4a3cb9bf8f20c9725b75",
            "170c76bd3a4a44a0ba1b21bf12d4448e",
            "201aa52a6a2745dfbd07d35e3ae62a9c",
            "b9443c6341ee4986998fe5de4c0bddea",
            "c981ac2acb1b42fc8b81e82fa304eb1d"
          ]
        },
        "id": "AcTUkw9Hbwex",
        "outputId": "11d39ed7-32df-4e6f-cbc8-573773683915"
      },
      "source": [
        "set_seed(42)    # Set seed for reproducibility\n",
        "bert_classifier, optimizer, scheduler = initialize_model(epochs=2)\n",
        "train(bert_classifier, train_dataloader, val_dataloader, epochs=2, evaluation=True)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "764f5c957aac4a3aa17129b3dbf2ddbf",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=433.0, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c55f8a845fd546bc938cd41a79c03417",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=440473133.0, style=ProgressStyle(descri…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Start training...\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
            "----------------------------------------------------------------------\n",
            "   1    |   20    |   0.319212   |     -      |     -     |   31.11  \n",
            "   1    |   40    |   0.239912   |     -      |     -     |   29.84  \n",
            "   1    |   60    |   0.218797   |     -      |     -     |   30.22  \n",
            "   1    |   80    |   0.187520   |     -      |     -     |   30.45  \n",
            "   1    |   100   |   0.172772   |     -      |     -     |   30.47  \n",
            "   1    |   120   |   0.183244   |     -      |     -     |   30.44  \n",
            "   1    |   140   |   0.154699   |     -      |     -     |   30.40  \n",
            "   1    |   160   |   0.104013   |     -      |     -     |   30.40  \n",
            "   1    |   180   |   0.125525   |     -      |     -     |   30.43  \n",
            "   1    |   200   |   0.171732   |     -      |     -     |   30.54  \n",
            "   1    |   220   |   0.166807   |     -      |     -     |   30.52  \n",
            "   1    |   240   |   0.130479   |     -      |     -     |   30.45  \n",
            "   1    |   260   |   0.158212   |     -      |     -     |   30.50  \n",
            "   1    |   280   |   0.154251   |     -      |     -     |   30.45  \n",
            "   1    |   300   |   0.122754   |     -      |     -     |   30.48  \n",
            "   1    |   320   |   0.117201   |     -      |     -     |   30.49  \n",
            "   1    |   340   |   0.161566   |     -      |     -     |   30.47  \n",
            "   1    |   360   |   0.140496   |     -      |     -     |   30.52  \n",
            "   1    |   380   |   0.164861   |     -      |     -     |   30.40  \n",
            "   1    |   400   |   0.248519   |     -      |     -     |   30.47  \n",
            "   1    |   420   |   0.118423   |     -      |     -     |   30.46  \n",
            "   1    |   440   |   0.094020   |     -      |     -     |   30.47  \n",
            "   1    |   460   |   0.148505   |     -      |     -     |   30.43  \n",
            "   1    |   480   |   0.156973   |     -      |     -     |   30.52  \n",
            "   1    |   500   |   0.119222   |     -      |     -     |   30.49  \n",
            "   1    |   520   |   0.117170   |     -      |     -     |   30.48  \n",
            "   1    |   540   |   0.100737   |     -      |     -     |   30.47  \n",
            "   1    |   560   |   0.168102   |     -      |     -     |   30.49  \n",
            "   1    |   580   |   0.111164   |     -      |     -     |   30.46  \n",
            "   1    |   600   |   0.110774   |     -      |     -     |   30.47  \n",
            "   1    |   620   |   0.140316   |     -      |     -     |   30.41  \n",
            "   1    |   640   |   0.137411   |     -      |     -     |   30.54  \n",
            "   1    |   660   |   0.137637   |     -      |     -     |   30.54  \n",
            "   1    |   680   |   0.108780   |     -      |     -     |   30.54  \n",
            "   1    |   700   |   0.098560   |     -      |     -     |   30.42  \n",
            "   1    |   720   |   0.107589   |     -      |     -     |   30.35  \n",
            "   1    |   740   |   0.111012   |     -      |     -     |   30.21  \n",
            "   1    |   760   |   0.121789   |     -      |     -     |   30.36  \n",
            "   1    |   780   |   0.135323   |     -      |     -     |   30.41  \n",
            "   1    |   800   |   0.110384   |     -      |     -     |   30.35  \n",
            "   1    |   820   |   0.144560   |     -      |     -     |   30.53  \n",
            "   1    |   840   |   0.097646   |     -      |     -     |   30.38  \n",
            "   1    |   860   |   0.085035   |     -      |     -     |   30.31  \n",
            "   1    |   880   |   0.153873   |     -      |     -     |   30.27  \n",
            "   1    |   900   |   0.170725   |     -      |     -     |   30.59  \n",
            "   1    |   920   |   0.077572   |     -      |     -     |   30.13  \n",
            "   1    |   940   |   0.074010   |     -      |     -     |   29.82  \n",
            "   1    |   960   |   0.087689   |     -      |     -     |   29.92  \n",
            "   1    |   980   |   0.121591   |     -      |     -     |   30.29  \n",
            "   1    |  1000   |   0.165382   |     -      |     -     |   30.58  \n",
            "   1    |  1020   |   0.135525   |     -      |     -     |   30.48  \n",
            "   1    |  1040   |   0.153907   |     -      |     -     |   30.49  \n",
            "   1    |  1060   |   0.100405   |     -      |     -     |   30.50  \n",
            "   1    |  1080   |   0.126140   |     -      |     -     |   30.56  \n",
            "   1    |  1100   |   0.104379   |     -      |     -     |   30.29  \n",
            "   1    |  1120   |   0.118523   |     -      |     -     |   30.28  \n",
            "   1    |  1140   |   0.121388   |     -      |     -     |   30.15  \n",
            "   1    |  1160   |   0.136985   |     -      |     -     |   30.22  \n",
            "   1    |  1180   |   0.096993   |     -      |     -     |   30.16  \n",
            "   1    |  1200   |   0.083121   |     -      |     -     |   30.13  \n",
            "   1    |  1220   |   0.135134   |     -      |     -     |   30.20  \n",
            "   1    |  1240   |   0.085390   |     -      |     -     |   30.14  \n",
            "   1    |  1260   |   0.142704   |     -      |     -     |   30.17  \n",
            "   1    |  1280   |   0.130295   |     -      |     -     |   30.19  \n",
            "   1    |  1300   |   0.102828   |     -      |     -     |   30.15  \n",
            "   1    |  1320   |   0.137156   |     -      |     -     |   30.21  \n",
            "   1    |  1340   |   0.136042   |     -      |     -     |   30.11  \n",
            "   1    |  1360   |   0.107505   |     -      |     -     |   30.13  \n",
            "   1    |  1380   |   0.096240   |     -      |     -     |   30.12  \n",
            "   1    |  1400   |   0.106465   |     -      |     -     |   30.12  \n",
            "   1    |  1420   |   0.106434   |     -      |     -     |   30.12  \n",
            "   1    |  1440   |   0.175813   |     -      |     -     |   30.11  \n",
            "   1    |  1460   |   0.112168   |     -      |     -     |   30.18  \n",
            "   1    |  1480   |   0.127396   |     -      |     -     |   30.15  \n",
            "   1    |  1500   |   0.114695   |     -      |     -     |   30.13  \n",
            "   1    |  1520   |   0.124264   |     -      |     -     |   30.13  \n",
            "   1    |  1540   |   0.121918   |     -      |     -     |   30.19  \n",
            "   1    |  1560   |   0.127194   |     -      |     -     |   30.22  \n",
            "   1    |  1580   |   0.080671   |     -      |     -     |   30.15  \n",
            "   1    |  1600   |   0.084227   |     -      |     -     |   30.17  \n",
            "   1    |  1620   |   0.108116   |     -      |     -     |   30.18  \n",
            "   1    |  1640   |   0.127690   |     -      |     -     |   30.16  \n",
            "   1    |  1660   |   0.093373   |     -      |     -     |   30.16  \n",
            "   1    |  1680   |   0.088114   |     -      |     -     |   30.13  \n",
            "   1    |  1700   |   0.167493   |     -      |     -     |   30.16  \n",
            "   1    |  1720   |   0.111339   |     -      |     -     |   30.14  \n",
            "   1    |  1740   |   0.110216   |     -      |     -     |   30.12  \n",
            "   1    |  1760   |   0.115053   |     -      |     -     |   30.14  \n",
            "   1    |  1780   |   0.122341   |     -      |     -     |   30.21  \n",
            "   1    |  1800   |   0.134287   |     -      |     -     |   30.16  \n",
            "   1    |  1820   |   0.120271   |     -      |     -     |   30.17  \n",
            "   1    |  1840   |   0.066324   |     -      |     -     |   30.11  \n",
            "   1    |  1860   |   0.155913   |     -      |     -     |   30.17  \n",
            "   1    |  1880   |   0.165820   |     -      |     -     |   30.22  \n",
            "   1    |  1900   |   0.119862   |     -      |     -     |   30.17  \n",
            "   1    |  1920   |   0.130952   |     -      |     -     |   30.20  \n",
            "   1    |  1940   |   0.114849   |     -      |     -     |   30.18  \n",
            "   1    |  1960   |   0.103932   |     -      |     -     |   30.13  \n",
            "   1    |  1980   |   0.095315   |     -      |     -     |   30.16  \n",
            "   1    |  2000   |   0.054350   |     -      |     -     |   30.11  \n",
            "   1    |  2020   |   0.152342   |     -      |     -     |   30.21  \n",
            "   1    |  2040   |   0.114409   |     -      |     -     |   30.15  \n",
            "   1    |  2060   |   0.123983   |     -      |     -     |   30.21  \n",
            "   1    |  2080   |   0.118544   |     -      |     -     |   30.20  \n",
            "   1    |  2100   |   0.095654   |     -      |     -     |   30.06  \n",
            "   1    |  2120   |   0.060018   |     -      |     -     |   30.07  \n",
            "   1    |  2140   |   0.117759   |     -      |     -     |   30.09  \n",
            "   1    |  2160   |   0.148119   |     -      |     -     |   30.15  \n",
            "   1    |  2180   |   0.142433   |     -      |     -     |   30.16  \n",
            "   1    |  2200   |   0.149220   |     -      |     -     |   30.16  \n",
            "   1    |  2220   |   0.104048   |     -      |     -     |   30.10  \n",
            "   1    |  2240   |   0.127397   |     -      |     -     |   30.08  \n",
            "   1    |  2260   |   0.189234   |     -      |     -     |   30.22  \n",
            "   1    |  2280   |   0.110139   |     -      |     -     |   30.18  \n",
            "   1    |  2300   |   0.111032   |     -      |     -     |   30.10  \n",
            "   1    |  2320   |   0.128151   |     -      |     -     |   30.25  \n",
            "   1    |  2340   |   0.091150   |     -      |     -     |   30.18  \n",
            "   1    |  2360   |   0.130738   |     -      |     -     |   30.20  \n",
            "   1    |  2380   |   0.141024   |     -      |     -     |   30.15  \n",
            "   1    |  2400   |   0.114129   |     -      |     -     |   30.19  \n",
            "   1    |  2420   |   0.164445   |     -      |     -     |   30.14  \n",
            "   1    |  2440   |   0.103549   |     -      |     -     |   30.13  \n",
            "   1    |  2460   |   0.078685   |     -      |     -     |   30.14  \n",
            "   1    |  2480   |   0.088311   |     -      |     -     |   30.12  \n",
            "   1    |  2500   |   0.065325   |     -      |     -     |   30.19  \n",
            "   1    |  2520   |   0.161238   |     -      |     -     |   30.21  \n",
            "   1    |  2540   |   0.089916   |     -      |     -     |   30.17  \n",
            "   1    |  2560   |   0.149050   |     -      |     -     |   30.20  \n",
            "   1    |  2580   |   0.157308   |     -      |     -     |   30.16  \n",
            "   1    |  2600   |   0.099601   |     -      |     -     |   30.17  \n",
            "   1    |  2620   |   0.091025   |     -      |     -     |   30.15  \n",
            "   1    |  2640   |   0.118002   |     -      |     -     |   30.15  \n",
            "   1    |  2660   |   0.102206   |     -      |     -     |   30.08  \n",
            "   1    |  2680   |   0.080299   |     -      |     -     |   30.10  \n",
            "   1    |  2700   |   0.077279   |     -      |     -     |   30.30  \n",
            "   1    |  2720   |   0.117652   |     -      |     -     |   30.48  \n",
            "   1    |  2740   |   0.121553   |     -      |     -     |   30.34  \n",
            "   1    |  2760   |   0.134279   |     -      |     -     |   30.36  \n",
            "   1    |  2780   |   0.138635   |     -      |     -     |   30.46  \n",
            "   1    |  2800   |   0.121643   |     -      |     -     |   30.41  \n",
            "   1    |  2820   |   0.093849   |     -      |     -     |   30.22  \n",
            "   1    |  2840   |   0.107348   |     -      |     -     |   30.25  \n",
            "   1    |  2860   |   0.102956   |     -      |     -     |   30.13  \n",
            "   1    |  2880   |   0.131329   |     -      |     -     |   30.17  \n",
            "   1    |  2900   |   0.159108   |     -      |     -     |   30.12  \n",
            "   1    |  2920   |   0.118025   |     -      |     -     |   30.12  \n",
            "   1    |  2940   |   0.081537   |     -      |     -     |   30.16  \n",
            "   1    |  2960   |   0.099626   |     -      |     -     |   30.14  \n",
            "   1    |  2980   |   0.167312   |     -      |     -     |   30.09  \n",
            "   1    |  3000   |   0.104372   |     -      |     -     |   30.13  \n",
            "   1    |  3020   |   0.181325   |     -      |     -     |   30.19  \n",
            "   1    |  3040   |   0.150231   |     -      |     -     |   30.16  \n",
            "   1    |  3060   |   0.123755   |     -      |     -     |   30.22  \n",
            "   1    |  3080   |   0.105906   |     -      |     -     |   30.15  \n",
            "   1    |  3100   |   0.148375   |     -      |     -     |   30.14  \n",
            "   1    |  3120   |   0.082188   |     -      |     -     |   30.25  \n",
            "   1    |  3140   |   0.092827   |     -      |     -     |   30.12  \n",
            "   1    |  3160   |   0.128582   |     -      |     -     |   30.19  \n",
            "   1    |  3180   |   0.167236   |     -      |     -     |   30.12  \n",
            "   1    |  3200   |   0.178402   |     -      |     -     |   30.09  \n",
            "   1    |  3220   |   0.180895   |     -      |     -     |   30.14  \n",
            "   1    |  3240   |   0.180896   |     -      |     -     |   30.15  \n",
            "   1    |  3260   |   0.175628   |     -      |     -     |   30.10  \n",
            "   1    |  3280   |   0.130483   |     -      |     -     |   30.17  \n",
            "   1    |  3300   |   0.207504   |     -      |     -     |   30.17  \n",
            "   1    |  3320   |   0.151503   |     -      |     -     |   30.11  \n",
            "   1    |  3340   |   0.144797   |     -      |     -     |   30.20  \n",
            "   1    |  3360   |   0.127930   |     -      |     -     |   30.13  \n",
            "   1    |  3380   |   0.127876   |     -      |     -     |   30.13  \n",
            "   1    |  3400   |   0.109107   |     -      |     -     |   30.15  \n",
            "   1    |  3420   |   0.116410   |     -      |     -     |   30.19  \n",
            "   1    |  3440   |   0.138430   |     -      |     -     |   30.12  \n",
            "   1    |  3460   |   0.174260   |     -      |     -     |   30.18  \n",
            "   1    |  3480   |   0.138836   |     -      |     -     |   30.10  \n",
            "   1    |  3500   |   0.144033   |     -      |     -     |   30.33  \n",
            "   1    |  3520   |   0.136415   |     -      |     -     |   30.45  \n",
            "   1    |  3540   |   0.095367   |     -      |     -     |   30.54  \n",
            "   1    |  3560   |   0.162277   |     -      |     -     |   30.57  \n",
            "   1    |  3580   |   0.136794   |     -      |     -     |   30.55  \n",
            "   1    |  3600   |   0.162215   |     -      |     -     |   30.49  \n",
            "   1    |  3620   |   0.116082   |     -      |     -     |   30.45  \n",
            "   1    |  3640   |   0.075778   |     -      |     -     |   30.34  \n",
            "   1    |  3660   |   0.093279   |     -      |     -     |   30.18  \n",
            "   1    |  3680   |   0.102158   |     -      |     -     |   30.32  \n",
            "   1    |  3700   |   0.105142   |     -      |     -     |   30.43  \n",
            "   1    |  3720   |   0.103281   |     -      |     -     |   30.50  \n",
            "   1    |  3740   |   0.113895   |     -      |     -     |   30.42  \n",
            "   1    |  3760   |   0.143495   |     -      |     -     |   30.26  \n",
            "   1    |  3780   |   0.109131   |     -      |     -     |   30.36  \n",
            "   1    |  3800   |   0.150133   |     -      |     -     |   30.50  \n",
            "   1    |  3820   |   0.098935   |     -      |     -     |   30.41  \n",
            "   1    |  3840   |   0.086395   |     -      |     -     |   30.44  \n",
            "   1    |  3860   |   0.079258   |     -      |     -     |   30.44  \n",
            "   1    |  3880   |   0.102172   |     -      |     -     |   30.41  \n",
            "   1    |  3900   |   0.223227   |     -      |     -     |   30.48  \n",
            "   1    |  3920   |   0.176253   |     -      |     -     |   30.53  \n",
            "   1    |  3940   |   0.072949   |     -      |     -     |   30.47  \n",
            "   1    |  3960   |   0.204446   |     -      |     -     |   30.43  \n",
            "   1    |  3980   |   0.178610   |     -      |     -     |   30.51  \n",
            "   1    |  4000   |   0.142234   |     -      |     -     |   30.50  \n",
            "   1    |  4020   |   0.121128   |     -      |     -     |   30.44  \n",
            "   1    |  4040   |   0.152749   |     -      |     -     |   30.43  \n",
            "   1    |  4060   |   0.161486   |     -      |     -     |   30.51  \n",
            "   1    |  4080   |   0.151774   |     -      |     -     |   30.50  \n",
            "   1    |  4100   |   0.131151   |     -      |     -     |   30.49  \n",
            "   1    |  4120   |   0.110522   |     -      |     -     |   30.42  \n",
            "   1    |  4140   |   0.123773   |     -      |     -     |   30.47  \n",
            "   1    |  4160   |   0.155504   |     -      |     -     |   30.50  \n",
            "   1    |  4180   |   0.173523   |     -      |     -     |   30.50  \n",
            "   1    |  4200   |   0.088601   |     -      |     -     |   30.35  \n",
            "   1    |  4220   |   0.225847   |     -      |     -     |   30.43  \n",
            "   1    |  4240   |   0.210764   |     -      |     -     |   30.38  \n",
            "   1    |  4260   |   0.146183   |     -      |     -     |   30.35  \n",
            "   1    |  4280   |   0.184595   |     -      |     -     |   30.36  \n",
            "   1    |  4300   |   0.167081   |     -      |     -     |   30.42  \n",
            "   1    |  4320   |   0.179529   |     -      |     -     |   30.42  \n",
            "   1    |  4340   |   0.103529   |     -      |     -     |   30.38  \n",
            "   1    |  4360   |   0.187392   |     -      |     -     |   30.42  \n",
            "   1    |  4380   |   0.164982   |     -      |     -     |   30.39  \n",
            "   1    |  4400   |   0.144951   |     -      |     -     |   30.40  \n",
            "   1    |  4420   |   0.234783   |     -      |     -     |   30.35  \n",
            "   1    |  4440   |   0.172233   |     -      |     -     |   30.36  \n",
            "   1    |  4460   |   0.190766   |     -      |     -     |   30.38  \n",
            "   1    |  4480   |   0.273659   |     -      |     -     |   30.42  \n",
            "   1    |  4500   |   0.268836   |     -      |     -     |   30.35  \n",
            "   1    |  4520   |   0.225144   |     -      |     -     |   30.44  \n",
            "   1    |  4540   |   0.268743   |     -      |     -     |   30.38  \n",
            "   1    |  4560   |   0.182838   |     -      |     -     |   30.42  \n",
            "   1    |  4580   |   0.135194   |     -      |     -     |   30.42  \n",
            "   1    |  4600   |   0.166922   |     -      |     -     |   30.42  \n",
            "   1    |  4620   |   0.094497   |     -      |     -     |   30.39  \n",
            "   1    |  4640   |   0.161510   |     -      |     -     |   30.37  \n",
            "   1    |  4660   |   0.147236   |     -      |     -     |   30.38  \n",
            "   1    |  4680   |   0.213196   |     -      |     -     |   30.36  \n",
            "   1    |  4700   |   0.166756   |     -      |     -     |   30.42  \n",
            "   1    |  4720   |   0.158035   |     -      |     -     |   30.42  \n",
            "   1    |  4740   |   0.139544   |     -      |     -     |   30.43  \n",
            "   1    |  4760   |   0.209265   |     -      |     -     |   30.43  \n",
            "   1    |  4780   |   0.120226   |     -      |     -     |   30.39  \n",
            "   1    |  4800   |   0.116042   |     -      |     -     |   30.40  \n",
            "   1    |  4820   |   0.138633   |     -      |     -     |   30.43  \n",
            "   1    |  4840   |   0.340443   |     -      |     -     |   30.51  \n",
            "   1    |  4860   |   0.184635   |     -      |     -     |   30.44  \n",
            "   1    |  4880   |   0.139122   |     -      |     -     |   30.44  \n",
            "   1    |  4900   |   0.160159   |     -      |     -     |   30.39  \n",
            "   1    |  4920   |   0.183941   |     -      |     -     |   30.42  \n",
            "   1    |  4940   |   0.137484   |     -      |     -     |   30.51  \n",
            "   1    |  4960   |   0.114450   |     -      |     -     |   30.44  \n",
            "   1    |  4980   |   0.129268   |     -      |     -     |   30.41  \n",
            "   1    |  5000   |   0.170441   |     -      |     -     |   30.48  \n",
            "   1    |  5020   |   0.159399   |     -      |     -     |   30.44  \n",
            "   1    |  5040   |   0.124052   |     -      |     -     |   30.41  \n",
            "   1    |  5060   |   0.131568   |     -      |     -     |   30.43  \n",
            "   1    |  5080   |   0.168733   |     -      |     -     |   30.42  \n",
            "   1    |  5100   |   0.085531   |     -      |     -     |   30.38  \n",
            "   1    |  5120   |   0.138540   |     -      |     -     |   30.38  \n",
            "   1    |  5140   |   0.157540   |     -      |     -     |   30.51  \n",
            "   1    |  5160   |   0.125767   |     -      |     -     |   30.51  \n",
            "   1    |  5180   |   0.210574   |     -      |     -     |   30.41  \n",
            "   1    |  5200   |   0.119645   |     -      |     -     |   30.39  \n",
            "   1    |  5220   |   0.156888   |     -      |     -     |   30.42  \n",
            "   1    |  5240   |   0.176240   |     -      |     -     |   30.49  \n",
            "   1    |  5260   |   0.141459   |     -      |     -     |   30.46  \n",
            "   1    |  5280   |   0.125223   |     -      |     -     |   30.45  \n",
            "   1    |  5300   |   0.111690   |     -      |     -     |   30.47  \n",
            "   1    |  5320   |   0.115512   |     -      |     -     |   30.44  \n",
            "   1    |  5340   |   0.126770   |     -      |     -     |   30.42  \n",
            "   1    |  5360   |   0.134396   |     -      |     -     |   30.42  \n",
            "   1    |  5380   |   0.139329   |     -      |     -     |   30.42  \n",
            "   1    |  5400   |   0.176632   |     -      |     -     |   30.43  \n",
            "   1    |  5420   |   0.167135   |     -      |     -     |   30.37  \n",
            "   1    |  5440   |   0.160912   |     -      |     -     |   30.50  \n",
            "   1    |  5460   |   0.130359   |     -      |     -     |   30.48  \n",
            "   1    |  5480   |   0.090161   |     -      |     -     |   30.41  \n",
            "   1    |  5500   |   0.139602   |     -      |     -     |   30.30  \n",
            "   1    |  5520   |   0.135537   |     -      |     -     |   30.40  \n",
            "   1    |  5540   |   0.183899   |     -      |     -     |   30.43  \n",
            "   1    |  5560   |   0.116740   |     -      |     -     |   30.44  \n",
            "   1    |  5580   |   0.126952   |     -      |     -     |   30.40  \n",
            "   1    |  5600   |   0.166347   |     -      |     -     |   30.40  \n",
            "   1    |  5620   |   0.100021   |     -      |     -     |   30.43  \n",
            "   1    |  5640   |   0.085560   |     -      |     -     |   30.38  \n",
            "   1    |  5660   |   0.174725   |     -      |     -     |   30.41  \n",
            "   1    |  5680   |   0.141859   |     -      |     -     |   30.40  \n",
            "   1    |  5700   |   0.133807   |     -      |     -     |   30.33  \n",
            "   1    |  5720   |   0.164804   |     -      |     -     |   30.40  \n",
            "   1    |  5740   |   0.186004   |     -      |     -     |   30.29  \n",
            "   1    |  5760   |   0.178733   |     -      |     -     |   30.42  \n",
            "   1    |  5780   |   0.186092   |     -      |     -     |   30.32  \n",
            "   1    |  5800   |   0.194762   |     -      |     -     |   30.47  \n",
            "   1    |  5820   |   0.138827   |     -      |     -     |   30.40  \n",
            "   1    |  5840   |   0.132698   |     -      |     -     |   30.40  \n",
            "   1    |  5860   |   0.170935   |     -      |     -     |   30.43  \n",
            "   1    |  5880   |   0.110793   |     -      |     -     |   30.45  \n",
            "   1    |  5900   |   0.104177   |     -      |     -     |   30.43  \n",
            "   1    |  5920   |   0.129987   |     -      |     -     |   30.48  \n",
            "   1    |  5940   |   0.166188   |     -      |     -     |   30.55  \n",
            "   1    |  5960   |   0.092122   |     -      |     -     |   30.52  \n",
            "   1    |  5980   |   0.148265   |     -      |     -     |   30.52  \n",
            "   1    |  6000   |   0.145458   |     -      |     -     |   30.44  \n",
            "   1    |  6020   |   0.116473   |     -      |     -     |   30.43  \n",
            "   1    |  6040   |   0.131329   |     -      |     -     |   30.31  \n",
            "   1    |  6060   |   0.131372   |     -      |     -     |   30.33  \n",
            "   1    |  6080   |   0.161266   |     -      |     -     |   30.41  \n",
            "   1    |  6100   |   0.182778   |     -      |     -     |   30.39  \n",
            "   1    |  6120   |   0.202718   |     -      |     -     |   30.48  \n",
            "   1    |  6140   |   0.178650   |     -      |     -     |   30.39  \n",
            "   1    |  6160   |   0.136430   |     -      |     -     |   30.48  \n",
            "   1    |  6180   |   0.140920   |     -      |     -     |   30.41  \n",
            "   1    |  6200   |   0.135969   |     -      |     -     |   30.48  \n",
            "   1    |  6220   |   0.150877   |     -      |     -     |   30.46  \n",
            "   1    |  6240   |   0.153877   |     -      |     -     |   30.44  \n",
            "   1    |  6260   |   0.143296   |     -      |     -     |   30.45  \n",
            "   1    |  6280   |   0.214975   |     -      |     -     |   30.48  \n",
            "   1    |  6300   |   0.156044   |     -      |     -     |   30.51  \n",
            "   1    |  6320   |   0.097577   |     -      |     -     |   30.45  \n",
            "   1    |  6340   |   0.092095   |     -      |     -     |   30.45  \n",
            "   1    |  6360   |   0.218108   |     -      |     -     |   30.41  \n",
            "   1    |  6380   |   0.094730   |     -      |     -     |   30.41  \n",
            "   1    |  6400   |   0.130456   |     -      |     -     |   30.46  \n",
            "   1    |  6420   |   0.061153   |     -      |     -     |   30.40  \n",
            "   1    |  6440   |   0.200428   |     -      |     -     |   30.39  \n",
            "   1    |  6460   |   0.163856   |     -      |     -     |   30.49  \n",
            "   1    |  6480   |   0.223335   |     -      |     -     |   30.50  \n",
            "   1    |  6500   |   0.096961   |     -      |     -     |   30.39  \n",
            "   1    |  6520   |   0.134620   |     -      |     -     |   30.39  \n",
            "   1    |  6540   |   0.090391   |     -      |     -     |   30.42  \n",
            "   1    |  6560   |   0.118348   |     -      |     -     |   30.34  \n",
            "   1    |  6580   |   0.138680   |     -      |     -     |   30.41  \n",
            "   1    |  6600   |   0.164055   |     -      |     -     |   30.43  \n",
            "   1    |  6620   |   0.121018   |     -      |     -     |   30.41  \n",
            "   1    |  6640   |   0.108734   |     -      |     -     |   30.45  \n",
            "   1    |  6660   |   0.142318   |     -      |     -     |   30.54  \n",
            "   1    |  6680   |   0.195598   |     -      |     -     |   30.46  \n",
            "   1    |  6700   |   0.117019   |     -      |     -     |   30.44  \n",
            "   1    |  6720   |   0.172751   |     -      |     -     |   30.46  \n",
            "   1    |  6740   |   0.169965   |     -      |     -     |   30.46  \n",
            "   1    |  6760   |   0.158668   |     -      |     -     |   30.49  \n",
            "   1    |  6780   |   0.198339   |     -      |     -     |   30.43  \n",
            "   1    |  6800   |   0.146857   |     -      |     -     |   30.47  \n",
            "   1    |  6820   |   0.185691   |     -      |     -     |   30.49  \n",
            "   1    |  6840   |   0.125436   |     -      |     -     |   30.50  \n",
            "   1    |  6860   |   0.173517   |     -      |     -     |   30.50  \n",
            "   1    |  6880   |   0.173579   |     -      |     -     |   30.50  \n",
            "   1    |  6900   |   0.160574   |     -      |     -     |   30.50  \n",
            "   1    |  6920   |   0.168294   |     -      |     -     |   30.39  \n",
            "   1    |  6940   |   0.181610   |     -      |     -     |   30.36  \n",
            "   1    |  6960   |   0.149454   |     -      |     -     |   30.47  \n",
            "   1    |  6980   |   0.154026   |     -      |     -     |   30.60  \n",
            "   1    |  6981   |   0.075166   |     -      |     -     |   0.40   \n",
            "----------------------------------------------------------------------\n",
            "   1    |    -    |   0.139007   |  0.142706  |   95.83   | 12287.74 \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
            "----------------------------------------------------------------------\n",
            "   2    |   20    |   0.078343   |     -      |     -     |   31.55  \n",
            "   2    |   40    |   0.149791   |     -      |     -     |   30.20  \n",
            "   2    |   60    |   0.117500   |     -      |     -     |   30.18  \n",
            "   2    |   80    |   0.149075   |     -      |     -     |   30.11  \n",
            "   2    |   100   |   0.082000   |     -      |     -     |   30.07  \n",
            "   2    |   120   |   0.164396   |     -      |     -     |   30.16  \n",
            "   2    |   140   |   0.068100   |     -      |     -     |   30.13  \n",
            "   2    |   160   |   0.145154   |     -      |     -     |   30.12  \n",
            "   2    |   180   |   0.122570   |     -      |     -     |   30.11  \n",
            "   2    |   200   |   0.149080   |     -      |     -     |   30.14  \n",
            "   2    |   220   |   0.124077   |     -      |     -     |   30.13  \n",
            "   2    |   240   |   0.128802   |     -      |     -     |   30.22  \n",
            "   2    |   260   |   0.129345   |     -      |     -     |   30.01  \n",
            "   2    |   280   |   0.116148   |     -      |     -     |   29.95  \n",
            "   2    |   300   |   0.105078   |     -      |     -     |   29.90  \n",
            "   2    |   320   |   0.208315   |     -      |     -     |   29.83  \n",
            "   2    |   340   |   0.117082   |     -      |     -     |   29.77  \n",
            "   2    |   360   |   0.108489   |     -      |     -     |   29.77  \n",
            "   2    |   380   |   0.171953   |     -      |     -     |   29.86  \n",
            "   2    |   400   |   0.157926   |     -      |     -     |   29.85  \n",
            "   2    |   420   |   0.147231   |     -      |     -     |   29.92  \n",
            "   2    |   440   |   0.088919   |     -      |     -     |   29.81  \n",
            "   2    |   460   |   0.163733   |     -      |     -     |   29.79  \n",
            "   2    |   480   |   0.101287   |     -      |     -     |   29.73  \n",
            "   2    |   500   |   0.086779   |     -      |     -     |   29.78  \n",
            "   2    |   520   |   0.149353   |     -      |     -     |   29.76  \n",
            "   2    |   540   |   0.158002   |     -      |     -     |   29.86  \n",
            "   2    |   560   |   0.168252   |     -      |     -     |   29.84  \n",
            "   2    |   580   |   0.098402   |     -      |     -     |   29.83  \n",
            "   2    |   600   |   0.156439   |     -      |     -     |   29.84  \n",
            "   2    |   620   |   0.093060   |     -      |     -     |   29.86  \n",
            "   2    |   640   |   0.163694   |     -      |     -     |   29.90  \n",
            "   2    |   660   |   0.159997   |     -      |     -     |   29.86  \n",
            "   2    |   680   |   0.095830   |     -      |     -     |   29.90  \n",
            "   2    |   700   |   0.085849   |     -      |     -     |   29.89  \n",
            "   2    |   720   |   0.117399   |     -      |     -     |   29.77  \n",
            "   2    |   740   |   0.117225   |     -      |     -     |   30.00  \n",
            "   2    |   760   |   0.126394   |     -      |     -     |   30.20  \n",
            "   2    |   780   |   0.144809   |     -      |     -     |   30.14  \n",
            "   2    |   800   |   0.122813   |     -      |     -     |   30.20  \n",
            "   2    |   820   |   0.080688   |     -      |     -     |   30.18  \n",
            "   2    |   840   |   0.105467   |     -      |     -     |   30.18  \n",
            "   2    |   860   |   0.158618   |     -      |     -     |   30.18  \n",
            "   2    |   880   |   0.065850   |     -      |     -     |   30.13  \n",
            "   2    |   900   |   0.150875   |     -      |     -     |   30.12  \n",
            "   2    |   920   |   0.106642   |     -      |     -     |   30.12  \n",
            "   2    |   940   |   0.107344   |     -      |     -     |   30.14  \n",
            "   2    |   960   |   0.124918   |     -      |     -     |   30.20  \n",
            "   2    |   980   |   0.111578   |     -      |     -     |   30.17  \n",
            "   2    |  1000   |   0.130900   |     -      |     -     |   30.16  \n",
            "   2    |  1020   |   0.154507   |     -      |     -     |   30.08  \n",
            "   2    |  1040   |   0.118463   |     -      |     -     |   30.00  \n",
            "   2    |  1060   |   0.112167   |     -      |     -     |   30.15  \n",
            "   2    |  1080   |   0.131263   |     -      |     -     |   30.16  \n",
            "   2    |  1100   |   0.094062   |     -      |     -     |   30.16  \n",
            "   2    |  1120   |   0.154188   |     -      |     -     |   30.09  \n",
            "   2    |  1140   |   0.104680   |     -      |     -     |   30.12  \n",
            "   2    |  1160   |   0.120062   |     -      |     -     |   30.14  \n",
            "   2    |  1180   |   0.165575   |     -      |     -     |   30.09  \n",
            "   2    |  1200   |   0.127805   |     -      |     -     |   30.12  \n",
            "   2    |  1220   |   0.079679   |     -      |     -     |   30.11  \n",
            "   2    |  1240   |   0.175492   |     -      |     -     |   30.12  \n",
            "   2    |  1260   |   0.108768   |     -      |     -     |   30.15  \n",
            "   2    |  1280   |   0.108302   |     -      |     -     |   30.09  \n",
            "   2    |  1300   |   0.122602   |     -      |     -     |   30.14  \n",
            "   2    |  1320   |   0.111608   |     -      |     -     |   30.13  \n",
            "   2    |  1340   |   0.208080   |     -      |     -     |   30.10  \n",
            "   2    |  1360   |   0.138678   |     -      |     -     |   30.13  \n",
            "   2    |  1380   |   0.102539   |     -      |     -     |   30.12  \n",
            "   2    |  1400   |   0.116733   |     -      |     -     |   30.07  \n",
            "   2    |  1420   |   0.094916   |     -      |     -     |   30.10  \n",
            "   2    |  1440   |   0.188504   |     -      |     -     |   30.14  \n",
            "   2    |  1460   |   0.101482   |     -      |     -     |   30.12  \n",
            "   2    |  1480   |   0.110397   |     -      |     -     |   30.12  \n",
            "   2    |  1500   |   0.175416   |     -      |     -     |   30.07  \n",
            "   2    |  1520   |   0.147376   |     -      |     -     |   30.09  \n",
            "   2    |  1540   |   0.124990   |     -      |     -     |   30.14  \n",
            "   2    |  1560   |   0.113804   |     -      |     -     |   30.13  \n",
            "   2    |  1580   |   0.108857   |     -      |     -     |   30.10  \n",
            "   2    |  1600   |   0.112899   |     -      |     -     |   30.07  \n",
            "   2    |  1620   |   0.105229   |     -      |     -     |   30.13  \n",
            "   2    |  1640   |   0.123654   |     -      |     -     |   30.15  \n",
            "   2    |  1660   |   0.068566   |     -      |     -     |   30.05  \n",
            "   2    |  1680   |   0.176717   |     -      |     -     |   30.14  \n",
            "   2    |  1700   |   0.108296   |     -      |     -     |   30.07  \n",
            "   2    |  1720   |   0.116445   |     -      |     -     |   30.11  \n",
            "   2    |  1740   |   0.109715   |     -      |     -     |   30.12  \n",
            "   2    |  1760   |   0.110833   |     -      |     -     |   30.12  \n",
            "   2    |  1780   |   0.131076   |     -      |     -     |   30.10  \n",
            "   2    |  1800   |   0.096073   |     -      |     -     |   30.07  \n",
            "   2    |  1820   |   0.082185   |     -      |     -     |   30.09  \n",
            "   2    |  1840   |   0.143961   |     -      |     -     |   30.12  \n",
            "   2    |  1860   |   0.110669   |     -      |     -     |   30.05  \n",
            "   2    |  1880   |   0.133274   |     -      |     -     |   30.14  \n",
            "   2    |  1900   |   0.151458   |     -      |     -     |   30.06  \n",
            "   2    |  1920   |   0.077330   |     -      |     -     |   30.07  \n",
            "   2    |  1940   |   0.171097   |     -      |     -     |   30.10  \n",
            "   2    |  1960   |   0.091165   |     -      |     -     |   30.02  \n",
            "   2    |  1980   |   0.088080   |     -      |     -     |   30.09  \n",
            "   2    |  2000   |   0.129399   |     -      |     -     |   30.08  \n",
            "   2    |  2020   |   0.097978   |     -      |     -     |   30.09  \n",
            "   2    |  2040   |   0.132274   |     -      |     -     |   30.15  \n",
            "   2    |  2060   |   0.086261   |     -      |     -     |   30.11  \n",
            "   2    |  2080   |   0.092408   |     -      |     -     |   30.07  \n",
            "   2    |  2100   |   0.155300   |     -      |     -     |   30.14  \n",
            "   2    |  2120   |   0.091769   |     -      |     -     |   30.05  \n",
            "   2    |  2140   |   0.208692   |     -      |     -     |   30.12  \n",
            "   2    |  2160   |   0.084402   |     -      |     -     |   30.12  \n",
            "   2    |  2180   |   0.061443   |     -      |     -     |   30.11  \n",
            "   2    |  2200   |   0.168849   |     -      |     -     |   30.13  \n",
            "   2    |  2220   |   0.088171   |     -      |     -     |   30.12  \n",
            "   2    |  2240   |   0.108235   |     -      |     -     |   30.12  \n",
            "   2    |  2260   |   0.098730   |     -      |     -     |   30.07  \n",
            "   2    |  2280   |   0.154470   |     -      |     -     |   30.13  \n",
            "   2    |  2300   |   0.174793   |     -      |     -     |   30.12  \n",
            "   2    |  2320   |   0.105053   |     -      |     -     |   30.07  \n",
            "   2    |  2340   |   0.108854   |     -      |     -     |   30.10  \n",
            "   2    |  2360   |   0.097789   |     -      |     -     |   30.13  \n",
            "   2    |  2380   |   0.177747   |     -      |     -     |   30.09  \n",
            "   2    |  2400   |   0.086991   |     -      |     -     |   30.11  \n",
            "   2    |  2420   |   0.123996   |     -      |     -     |   30.11  \n",
            "   2    |  2440   |   0.065739   |     -      |     -     |   30.11  \n",
            "   2    |  2460   |   0.114775   |     -      |     -     |   30.01  \n",
            "   2    |  2480   |   0.141336   |     -      |     -     |   30.05  \n",
            "   2    |  2500   |   0.114530   |     -      |     -     |   30.04  \n",
            "   2    |  2520   |   0.114305   |     -      |     -     |   29.99  \n",
            "   2    |  2540   |   0.120557   |     -      |     -     |   30.05  \n",
            "   2    |  2560   |   0.113722   |     -      |     -     |   30.04  \n",
            "   2    |  2580   |   0.103066   |     -      |     -     |   30.11  \n",
            "   2    |  2600   |   0.070200   |     -      |     -     |   30.06  \n",
            "   2    |  2620   |   0.166496   |     -      |     -     |   30.07  \n",
            "   2    |  2640   |   0.098635   |     -      |     -     |   30.08  \n",
            "   2    |  2660   |   0.123696   |     -      |     -     |   30.06  \n",
            "   2    |  2680   |   0.167863   |     -      |     -     |   30.08  \n",
            "   2    |  2700   |   0.115448   |     -      |     -     |   30.07  \n",
            "   2    |  2720   |   0.130511   |     -      |     -     |   30.04  \n",
            "   2    |  2740   |   0.127086   |     -      |     -     |   29.96  \n",
            "   2    |  2760   |   0.075973   |     -      |     -     |   29.98  \n",
            "   2    |  2780   |   0.069923   |     -      |     -     |   30.08  \n",
            "   2    |  2800   |   0.114915   |     -      |     -     |   30.09  \n",
            "   2    |  2820   |   0.077996   |     -      |     -     |   30.09  \n",
            "   2    |  2840   |   0.070687   |     -      |     -     |   30.06  \n",
            "   2    |  2860   |   0.106808   |     -      |     -     |   30.10  \n",
            "   2    |  2880   |   0.123499   |     -      |     -     |   30.08  \n",
            "   2    |  2900   |   0.099657   |     -      |     -     |   30.09  \n",
            "   2    |  2920   |   0.107017   |     -      |     -     |   30.04  \n",
            "   2    |  2940   |   0.077965   |     -      |     -     |   30.03  \n",
            "   2    |  2960   |   0.089826   |     -      |     -     |   30.09  \n",
            "   2    |  2980   |   0.105887   |     -      |     -     |   30.08  \n",
            "   2    |  3000   |   0.177101   |     -      |     -     |   30.13  \n",
            "   2    |  3020   |   0.105706   |     -      |     -     |   30.05  \n",
            "   2    |  3040   |   0.103184   |     -      |     -     |   30.03  \n",
            "   2    |  3060   |   0.093647   |     -      |     -     |   30.12  \n",
            "   2    |  3080   |   0.068446   |     -      |     -     |   30.10  \n",
            "   2    |  3100   |   0.115038   |     -      |     -     |   30.09  \n",
            "   2    |  3120   |   0.163323   |     -      |     -     |   30.04  \n",
            "   2    |  3140   |   0.094465   |     -      |     -     |   30.03  \n",
            "   2    |  3160   |   0.107453   |     -      |     -     |   30.05  \n",
            "   2    |  3180   |   0.137522   |     -      |     -     |   30.10  \n",
            "   2    |  3200   |   0.105935   |     -      |     -     |   30.13  \n",
            "   2    |  3220   |   0.132320   |     -      |     -     |   30.11  \n",
            "   2    |  3240   |   0.097233   |     -      |     -     |   30.13  \n",
            "   2    |  3260   |   0.101884   |     -      |     -     |   30.11  \n",
            "   2    |  3280   |   0.067273   |     -      |     -     |   30.07  \n",
            "   2    |  3300   |   0.043200   |     -      |     -     |   30.05  \n",
            "   2    |  3320   |   0.106496   |     -      |     -     |   30.15  \n",
            "   2    |  3340   |   0.147974   |     -      |     -     |   30.10  \n",
            "   2    |  3360   |   0.099990   |     -      |     -     |   30.09  \n",
            "   2    |  3380   |   0.040326   |     -      |     -     |   30.05  \n",
            "   2    |  3400   |   0.194626   |     -      |     -     |   30.18  \n",
            "   2    |  3420   |   0.164576   |     -      |     -     |   30.15  \n",
            "   2    |  3440   |   0.169266   |     -      |     -     |   30.04  \n",
            "   2    |  3460   |   0.115641   |     -      |     -     |   30.11  \n",
            "   2    |  3480   |   0.106825   |     -      |     -     |   30.14  \n",
            "   2    |  3500   |   0.101027   |     -      |     -     |   30.07  \n",
            "   2    |  3520   |   0.102040   |     -      |     -     |   30.20  \n",
            "   2    |  3540   |   0.074683   |     -      |     -     |   30.42  \n",
            "   2    |  3560   |   0.142086   |     -      |     -     |   30.49  \n",
            "   2    |  3580   |   0.152070   |     -      |     -     |   30.46  \n",
            "   2    |  3600   |   0.091500   |     -      |     -     |   30.42  \n",
            "   2    |  3620   |   0.187258   |     -      |     -     |   30.44  \n",
            "   2    |  3640   |   0.153751   |     -      |     -     |   30.45  \n",
            "   2    |  3660   |   0.229647   |     -      |     -     |   30.47  \n",
            "   2    |  3680   |   0.113470   |     -      |     -     |   30.44  \n",
            "   2    |  3700   |   0.110676   |     -      |     -     |   30.43  \n",
            "   2    |  3720   |   0.167135   |     -      |     -     |   30.41  \n",
            "   2    |  3740   |   0.138716   |     -      |     -     |   30.45  \n",
            "   2    |  3760   |   0.213589   |     -      |     -     |   30.45  \n",
            "   2    |  3780   |   0.152435   |     -      |     -     |   30.44  \n",
            "   2    |  3800   |   0.134662   |     -      |     -     |   30.45  \n",
            "   2    |  3820   |   0.150629   |     -      |     -     |   30.47  \n",
            "   2    |  3840   |   0.108193   |     -      |     -     |   30.40  \n",
            "   2    |  3860   |   0.101206   |     -      |     -     |   30.42  \n",
            "   2    |  3880   |   0.196141   |     -      |     -     |   30.47  \n",
            "   2    |  3900   |   0.110782   |     -      |     -     |   30.39  \n",
            "   2    |  3920   |   0.072132   |     -      |     -     |   30.43  \n",
            "   2    |  3940   |   0.134929   |     -      |     -     |   30.49  \n",
            "   2    |  3960   |   0.089246   |     -      |     -     |   30.43  \n",
            "   2    |  3980   |   0.066691   |     -      |     -     |   30.48  \n",
            "   2    |  4000   |   0.091148   |     -      |     -     |   30.50  \n",
            "   2    |  4020   |   0.122744   |     -      |     -     |   30.51  \n",
            "   2    |  4040   |   0.057022   |     -      |     -     |   30.43  \n",
            "   2    |  4060   |   0.181657   |     -      |     -     |   30.46  \n",
            "   2    |  4080   |   0.134569   |     -      |     -     |   30.44  \n",
            "   2    |  4100   |   0.051192   |     -      |     -     |   30.40  \n",
            "   2    |  4120   |   0.104872   |     -      |     -     |   30.45  \n",
            "   2    |  4140   |   0.174489   |     -      |     -     |   30.44  \n",
            "   2    |  4160   |   0.120787   |     -      |     -     |   30.45  \n",
            "   2    |  4180   |   0.151129   |     -      |     -     |   30.42  \n",
            "   2    |  4200   |   0.114188   |     -      |     -     |   30.46  \n",
            "   2    |  4220   |   0.077403   |     -      |     -     |   30.44  \n",
            "   2    |  4240   |   0.087714   |     -      |     -     |   30.49  \n",
            "   2    |  4260   |   0.054720   |     -      |     -     |   30.48  \n",
            "   2    |  4280   |   0.087476   |     -      |     -     |   30.38  \n",
            "   2    |  4300   |   0.093346   |     -      |     -     |   30.41  \n",
            "   2    |  4320   |   0.149559   |     -      |     -     |   30.47  \n",
            "   2    |  4340   |   0.086174   |     -      |     -     |   30.43  \n",
            "   2    |  4360   |   0.118340   |     -      |     -     |   30.43  \n",
            "   2    |  4380   |   0.105947   |     -      |     -     |   30.43  \n",
            "   2    |  4400   |   0.082586   |     -      |     -     |   30.46  \n",
            "   2    |  4420   |   0.150864   |     -      |     -     |   30.44  \n",
            "   2    |  4440   |   0.104098   |     -      |     -     |   30.40  \n",
            "   2    |  4460   |   0.098852   |     -      |     -     |   30.47  \n",
            "   2    |  4480   |   0.176906   |     -      |     -     |   30.52  \n",
            "   2    |  4500   |   0.088104   |     -      |     -     |   30.48  \n",
            "   2    |  4520   |   0.057684   |     -      |     -     |   30.41  \n",
            "   2    |  4540   |   0.099944   |     -      |     -     |   30.46  \n",
            "   2    |  4560   |   0.086817   |     -      |     -     |   30.44  \n",
            "   2    |  4580   |   0.169417   |     -      |     -     |   30.43  \n",
            "   2    |  4600   |   0.070693   |     -      |     -     |   30.48  \n",
            "   2    |  4620   |   0.122584   |     -      |     -     |   30.44  \n",
            "   2    |  4640   |   0.089849   |     -      |     -     |   30.39  \n",
            "   2    |  4660   |   0.108541   |     -      |     -     |   30.40  \n",
            "   2    |  4680   |   0.130963   |     -      |     -     |   30.45  \n",
            "   2    |  4700   |   0.114302   |     -      |     -     |   30.46  \n",
            "   2    |  4720   |   0.079861   |     -      |     -     |   30.45  \n",
            "   2    |  4740   |   0.125882   |     -      |     -     |   30.45  \n",
            "   2    |  4760   |   0.101919   |     -      |     -     |   30.49  \n",
            "   2    |  4780   |   0.121607   |     -      |     -     |   30.49  \n",
            "   2    |  4800   |   0.131599   |     -      |     -     |   30.46  \n",
            "   2    |  4820   |   0.115838   |     -      |     -     |   30.44  \n",
            "   2    |  4840   |   0.095876   |     -      |     -     |   30.47  \n",
            "   2    |  4860   |   0.129495   |     -      |     -     |   30.48  \n",
            "   2    |  4880   |   0.117104   |     -      |     -     |   30.46  \n",
            "   2    |  4900   |   0.099413   |     -      |     -     |   30.48  \n",
            "   2    |  4920   |   0.046670   |     -      |     -     |   30.47  \n",
            "   2    |  4940   |   0.096583   |     -      |     -     |   30.42  \n",
            "   2    |  4960   |   0.044528   |     -      |     -     |   30.33  \n",
            "   2    |  4980   |   0.126628   |     -      |     -     |   30.11  \n",
            "   2    |  5000   |   0.142372   |     -      |     -     |   30.12  \n",
            "   2    |  5020   |   0.116517   |     -      |     -     |   30.36  \n",
            "   2    |  5040   |   0.065669   |     -      |     -     |   30.37  \n",
            "   2    |  5060   |   0.087172   |     -      |     -     |   30.39  \n",
            "   2    |  5080   |   0.108934   |     -      |     -     |   30.37  \n",
            "   2    |  5100   |   0.138574   |     -      |     -     |   30.37  \n",
            "   2    |  5120   |   0.110073   |     -      |     -     |   30.45  \n",
            "   2    |  5140   |   0.062276   |     -      |     -     |   30.02  \n",
            "   2    |  5160   |   0.117494   |     -      |     -     |   29.69  \n",
            "   2    |  5180   |   0.105771   |     -      |     -     |   29.68  \n",
            "   2    |  5200   |   0.098513   |     -      |     -     |   29.71  \n",
            "   2    |  5220   |   0.189353   |     -      |     -     |   29.77  \n",
            "   2    |  5240   |   0.070757   |     -      |     -     |   29.74  \n",
            "   2    |  5260   |   0.118667   |     -      |     -     |   29.64  \n",
            "   2    |  5280   |   0.050665   |     -      |     -     |   29.62  \n",
            "   2    |  5300   |   0.092905   |     -      |     -     |   29.72  \n",
            "   2    |  5320   |   0.062026   |     -      |     -     |   29.70  \n",
            "   2    |  5340   |   0.042723   |     -      |     -     |   29.64  \n",
            "   2    |  5360   |   0.070975   |     -      |     -     |   29.67  \n",
            "   2    |  5380   |   0.059263   |     -      |     -     |   29.67  \n",
            "   2    |  5400   |   0.069066   |     -      |     -     |   29.67  \n",
            "   2    |  5420   |   0.093081   |     -      |     -     |   29.73  \n",
            "   2    |  5440   |   0.127457   |     -      |     -     |   29.74  \n",
            "   2    |  5460   |   0.149269   |     -      |     -     |   29.73  \n",
            "   2    |  5480   |   0.119962   |     -      |     -     |   29.71  \n",
            "   2    |  5500   |   0.134077   |     -      |     -     |   29.72  \n",
            "   2    |  5520   |   0.143112   |     -      |     -     |   29.81  \n",
            "   2    |  5540   |   0.083576   |     -      |     -     |   29.68  \n",
            "   2    |  5560   |   0.096834   |     -      |     -     |   29.64  \n",
            "   2    |  5580   |   0.082684   |     -      |     -     |   29.71  \n",
            "   2    |  5600   |   0.072886   |     -      |     -     |   29.68  \n",
            "   2    |  5620   |   0.093679   |     -      |     -     |   29.75  \n",
            "   2    |  5640   |   0.128001   |     -      |     -     |   29.72  \n",
            "   2    |  5660   |   0.091113   |     -      |     -     |   29.67  \n",
            "   2    |  5680   |   0.132156   |     -      |     -     |   29.64  \n",
            "   2    |  5700   |   0.059063   |     -      |     -     |   29.53  \n",
            "   2    |  5720   |   0.126639   |     -      |     -     |   29.65  \n",
            "   2    |  5740   |   0.098094   |     -      |     -     |   29.69  \n",
            "   2    |  5760   |   0.087313   |     -      |     -     |   29.66  \n",
            "   2    |  5780   |   0.094076   |     -      |     -     |   29.67  \n",
            "   2    |  5800   |   0.158566   |     -      |     -     |   29.66  \n",
            "   2    |  5820   |   0.115895   |     -      |     -     |   29.65  \n",
            "   2    |  5840   |   0.117012   |     -      |     -     |   29.69  \n",
            "   2    |  5860   |   0.059945   |     -      |     -     |   29.93  \n",
            "   2    |  5880   |   0.047146   |     -      |     -     |   30.18  \n",
            "   2    |  5900   |   0.089367   |     -      |     -     |   30.52  \n",
            "   2    |  5920   |   0.085968   |     -      |     -     |   30.48  \n",
            "   2    |  5940   |   0.107488   |     -      |     -     |   30.52  \n",
            "   2    |  5960   |   0.085255   |     -      |     -     |   30.55  \n",
            "   2    |  5980   |   0.109644   |     -      |     -     |   30.54  \n",
            "   2    |  6000   |   0.092891   |     -      |     -     |   30.45  \n",
            "   2    |  6020   |   0.072429   |     -      |     -     |   30.53  \n",
            "   2    |  6040   |   0.099390   |     -      |     -     |   30.47  \n",
            "   2    |  6060   |   0.119354   |     -      |     -     |   30.44  \n",
            "   2    |  6080   |   0.115304   |     -      |     -     |   30.42  \n",
            "   2    |  6100   |   0.115065   |     -      |     -     |   30.46  \n",
            "   2    |  6120   |   0.070461   |     -      |     -     |   30.45  \n",
            "   2    |  6140   |   0.178073   |     -      |     -     |   30.39  \n",
            "   2    |  6160   |   0.105596   |     -      |     -     |   30.46  \n",
            "   2    |  6180   |   0.067438   |     -      |     -     |   30.38  \n",
            "   2    |  6200   |   0.136999   |     -      |     -     |   30.47  \n",
            "   2    |  6220   |   0.089055   |     -      |     -     |   30.41  \n",
            "   2    |  6240   |   0.094861   |     -      |     -     |   30.37  \n",
            "   2    |  6260   |   0.108680   |     -      |     -     |   30.40  \n",
            "   2    |  6280   |   0.083100   |     -      |     -     |   30.18  \n",
            "   2    |  6300   |   0.083797   |     -      |     -     |   30.37  \n",
            "   2    |  6320   |   0.171509   |     -      |     -     |   30.41  \n",
            "   2    |  6340   |   0.096560   |     -      |     -     |   30.46  \n",
            "   2    |  6360   |   0.142075   |     -      |     -     |   30.40  \n",
            "   2    |  6380   |   0.085914   |     -      |     -     |   30.45  \n",
            "   2    |  6400   |   0.101475   |     -      |     -     |   30.37  \n",
            "   2    |  6420   |   0.085910   |     -      |     -     |   30.36  \n",
            "   2    |  6440   |   0.080341   |     -      |     -     |   30.41  \n",
            "   2    |  6460   |   0.055408   |     -      |     -     |   30.42  \n",
            "   2    |  6480   |   0.059709   |     -      |     -     |   30.39  \n",
            "   2    |  6500   |   0.105075   |     -      |     -     |   30.37  \n",
            "   2    |  6520   |   0.106516   |     -      |     -     |   30.38  \n",
            "   2    |  6540   |   0.047737   |     -      |     -     |   30.39  \n",
            "   2    |  6560   |   0.093399   |     -      |     -     |   30.40  \n",
            "   2    |  6580   |   0.101903   |     -      |     -     |   30.43  \n",
            "   2    |  6600   |   0.071329   |     -      |     -     |   30.38  \n",
            "   2    |  6620   |   0.079568   |     -      |     -     |   30.39  \n",
            "   2    |  6640   |   0.149507   |     -      |     -     |   30.42  \n",
            "   2    |  6660   |   0.102413   |     -      |     -     |   30.36  \n",
            "   2    |  6680   |   0.081339   |     -      |     -     |   30.41  \n",
            "   2    |  6700   |   0.115900   |     -      |     -     |   30.44  \n",
            "   2    |  6720   |   0.122487   |     -      |     -     |   30.45  \n",
            "   2    |  6740   |   0.092428   |     -      |     -     |   30.38  \n",
            "   2    |  6760   |   0.066067   |     -      |     -     |   30.40  \n",
            "   2    |  6780   |   0.068058   |     -      |     -     |   30.33  \n",
            "   2    |  6800   |   0.082924   |     -      |     -     |   30.40  \n",
            "   2    |  6820   |   0.044503   |     -      |     -     |   30.37  \n",
            "   2    |  6840   |   0.081524   |     -      |     -     |   30.38  \n",
            "   2    |  6860   |   0.082729   |     -      |     -     |   30.41  \n",
            "   2    |  6880   |   0.114851   |     -      |     -     |   30.44  \n",
            "   2    |  6900   |   0.110805   |     -      |     -     |   30.40  \n",
            "   2    |  6920   |   0.113056   |     -      |     -     |   30.38  \n",
            "   2    |  6940   |   0.121681   |     -      |     -     |   30.41  \n",
            "   2    |  6960   |   0.079960   |     -      |     -     |   30.43  \n",
            "   2    |  6980   |   0.075931   |     -      |     -     |   30.40  \n",
            "   2    |  6981   |   0.002125   |     -      |     -     |   0.38   \n",
            "----------------------------------------------------------------------\n",
            "   2    |    -    |   0.112441   |  0.116210  |   96.62   | 12232.91 \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Training complete!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VIBt0ARhbwhx"
      },
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "def bert_predict(model, test_dataloader):\n",
        "    \"\"\"Perform a forward pass on the trained BERT model to predict probabilities\n",
        "    on the test set.\n",
        "    \"\"\"\n",
        "    # Put the model into the evaluation mode. The dropout layers are disabled during\n",
        "    # the test time.\n",
        "    model.eval()\n",
        "\n",
        "    all_logits = []\n",
        "\n",
        "    # For each batch in our test set...\n",
        "    for batch in test_dataloader:\n",
        "        # Load batch to GPU\n",
        "        b_input_ids, b_attn_mask = tuple(t.to(device) for t in batch)[:2]\n",
        "\n",
        "        # Compute logits\n",
        "        with torch.no_grad():\n",
        "            logits = model(b_input_ids, b_attn_mask)\n",
        "        all_logits.append(logits)\n",
        "    \n",
        "    # Concatenate logits from each batch\n",
        "    all_logits = torch.cat(all_logits, dim=0)\n",
        "\n",
        "    # Apply softmax to calculate probabilities\n",
        "    probs = F.softmax(all_logits, dim=1).cpu().numpy()\n",
        "\n",
        "    return probs"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e03FcJzEbwky"
      },
      "source": [
        "from sklearn import metrics \n",
        "\n",
        "def evaluate_roc(predictions_prob_all, ground_truth):\n",
        "    \"\"\"\n",
        "    - Print AUC and accuracy on the test set\n",
        "    - Plot ROC\n",
        "    @params    probs (np.array): an array of predicted probabilities with shape (len(y_true), 2)\n",
        "    @params    y_true (np.array): an array of the true values with shape (len(y_true),)\n",
        "    \"\"\"\n",
        "    predictions_prob = predictions_prob_all[:, 1]\n",
        "    # fpr, tpr, threshold = roc_curve(y_true, preds)\n",
        "    # roc_auc = auc(fpr, tpr)\n",
        "    # print(f'AUC: {roc_auc:.4f}')\n",
        "       \n",
        "    # # Get accuracy over the test set\n",
        "    # y_pred = np.where(preds >= 0.5, 1, 0)\n",
        "    # accuracy = accuracy_score(y_true, y_pred)\n",
        "    # print(f'Accuracy: {accuracy*100:.2f}%')\n",
        "\n",
        "    ## Accuracy, Precision, Recall\n",
        "    predictions_labels = np.where(predictions_prob >= 0.5, 1, 0)\n",
        "    accuracy = metrics.accuracy_score(ground_truth, predictions_labels)\n",
        "    auc = metrics.roc_auc_score(ground_truth, predictions_prob)\n",
        "    print(\"Accuracy:\", round(accuracy, 2))\n",
        "    print(\"AUC:\", round(auc, 2))\n",
        "    print(\"Detail:\")\n",
        "    print(metrics.classification_report(ground_truth, predictions_labels))\n",
        "    \n",
        "    # # Plot ROC AUC\n",
        "    # plt.title('Receiver Operating Characteristic')\n",
        "    # plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
        "    # plt.legend(loc = 'lower right')\n",
        "    # plt.plot([0, 1], [0, 1],'r--')\n",
        "    # plt.xlim([0, 1])\n",
        "    # plt.ylim([0, 1])\n",
        "    # plt.ylabel('True Positive Rate')\n",
        "    # plt.xlabel('False Positive Rate')\n",
        "    # plt.show()"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "axz1HaFLarOb",
        "outputId": "83157482-0a9c-4dca-cc38-cb923426fcfc"
      },
      "source": [
        "probs_train = bert_predict(bert_classifier, train_dataloader)\n",
        "evaluate_roc(probs_train, y_train)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.83\n",
            "AUC: 0.51\n",
            "Detail:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.91      0.91    100993\n",
            "           1       0.10      0.09      0.09     10706\n",
            "\n",
            "    accuracy                           0.83    111699\n",
            "   macro avg       0.50      0.50      0.50    111699\n",
            "weighted avg       0.83      0.83      0.83    111699\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N34SOMs3cxeA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c413f60-51f0-47d1-9ce0-74db68470316"
      },
      "source": [
        "probs_val = bert_predict(bert_classifier, val_dataloader)\n",
        "evaluate_roc(probs_val, y_val)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.97\n",
            "AUC: 0.98\n",
            "Detail:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.99      0.98     43284\n",
            "           1       0.86      0.77      0.81      4588\n",
            "\n",
            "    accuracy                           0.97     47872\n",
            "   macro avg       0.92      0.88      0.90     47872\n",
            "weighted avg       0.97      0.97      0.97     47872\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dNuUgWhsS7pd"
      },
      "source": [
        "# Convert other data types to torch.Tensor\n",
        "test_labels = torch.tensor(y_test)\n",
        "\n",
        "# For fine-tuning BERT, the authors recommend a batch size of 16 or 32.\n",
        "batch_size = 16\n",
        "\n",
        "# Create the DataLoader for our test set\n",
        "test_data = TensorDataset(test_inputs, test_masks, test_labels)\n",
        "test_sampler = RandomSampler(test_data)\n",
        "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wtGo1ULrvrGY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74e5ffc9-f2ba-4a03-ce35-29989779247c"
      },
      "source": [
        "probs_test = bert_predict(bert_classifier, test_dataloader)\n",
        "evaluate_roc(probs_test, y_test)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.79\n",
            "AUC: 0.5\n",
            "Detail:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.86      0.88     57888\n",
            "           1       0.09      0.14      0.11      6090\n",
            "\n",
            "    accuracy                           0.79     63978\n",
            "   macro avg       0.50      0.50      0.50     63978\n",
            "weighted avg       0.83      0.79      0.81     63978\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QT98i-4x6MAJ"
      },
      "source": [
        "torch.save(bert_classifier, '/content/drive/MyDrive/ColabNotebooks/DE_ML_project/bert_model_2')"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GtuYxKRLlFiD"
      },
      "source": [
        "torch.save(bert_classifier.state_dict(), '/content/drive/MyDrive/ColabNotebooks/DE_ML_project/bert_model_2_saved_weights.pt')"
      ],
      "execution_count": 17,
      "outputs": []
    }
  ]
}